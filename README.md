<h1 align="center">ğŸ§  Pipeline ETL Temps RÃ©el<br>Redpanda â€¢ PySpark â€¢ MySQL â€¢ Docker</h1>

---

## ğŸ“Œ Objectif du projet <br>
Ce projet met en place un pipeline ETL temps rÃ©el permettant de :
 * GÃ©nÃ©rer des tickets clients via un producteur Python
 * Les envoyer dans un broker Kafka-compatible (Redpanda)
 * Les traiter en continu avec PySpark Structured Streaming
 * Stocker les rÃ©sultats dans MySQL
 * Exporter les donnÃ©es transformÃ©es en JSON et Parquet

ğŸ’¡ Ce POC dÃ©montre une architecture moderne de streaming temps rÃ©el, entiÃ¨rement conteneurisÃ©e avec Docker.

 ----

## âš™ï¸ Architecture du pipeline <br>
Voici une vue dâ€™ensemble du flux de donnÃ©es :
      
```mermaid
flowchart LR

A[Generator Tickets Python] --> B[Redpanda Kafka]
B --> C[PySpark Streaming]
C --> D[MySQL]
C --> E[Export JSON/Parquet]

D --> F[Dashboard / Analyse]
E --> F
```


------

## ğŸ§© Technologies utilisÃ©es <br>
             +----------------------+------------------------------+
             |  Composant	        |  Technologie                 |
             +----------------------+------------------------------+
             | Broker de messages	| Redpanda (Kafka API)         |
             | Traitement temps rÃ©el| PySpark (Spark 3.5)          |
             |  Base de donnÃ©es	    |  MySQL 8                     |
             | Langage	            |  Python 3                    |
             | Conteneurisation	    |  Docker & Docker Compose     |
             |  Export	            |  JSON & Parquet              |
             +----------------------+------------------------------+

## ğŸš€ Lancer le projet <br>    
   1. Cloner le projet
                   git clone <repo>
                   cd projet         
   2. Lancer lâ€™infrastructure
                   docker-compose up --build                


Les services suivants dÃ©marrent :redpanda,mysql,cheik-producer,cheik-spark

## ğŸ”„ Pipeline ETL : fonctionnement <br>
      1. Extraction

         *  Le script Python produce_tickets.py gÃ©nÃ¨re des tickets clients 
         *  Les messages sont envoyÃ©s dans le topic Kafka : client_tickets

      2. Transformation (PySpark): PySpark Structured Streaming :

        - lit les messages depuis Redpanda
        - parse le JSON
        - ajoute des colonnes (ex : Ã©quipe de support)
        - calcule des agrÃ©gations :nombre de tickets par type, nombre de tickets par Ã©quipe, statistiques temporelles

      3. Chargement:Les rÃ©sultats sont :

          - stockÃ©s dans MySQL (tickets, ticket_stats)
          - exportÃ©s en fichiers :(/output/json/),(/output/parquet/ )

------
## ğŸ“¦ Export des donnÃ©es <br>
- Les donnÃ©es transformÃ©es sont disponibles dans :(/output/json/),(/output/parquet/)
- Elles peuvent Ãªtre utilisÃ©es dans :Power BI,Tableau,Pandas,Spark SQL,Jupyter Notebook

-----
## ğŸ—„ï¸ VÃ©rifier les donnÃ©es dans MySQL <br>

          docker exec -it mysql mysql -u root -p
          USE ticketsdb;
          SELECT * FROM tickets LIMIT 10;
          SELECT * FROM ticket_stats LIMIT 10;
-----
## ğŸ¥ DÃ©monstration vidÃ©o <br>  


-----
## ğŸ“ Structure du projet <br>
                
                â”œâ”€â”€ producer/
                â”‚   â”œâ”€â”€ Dockerfile
                â”‚   â””â”€â”€ produce_tickets.py
                â”œâ”€â”€ spark/
                â”‚   â”œâ”€â”€ Dockerfile
                â”‚   â””â”€â”€ stream_tickets.py
                â”œâ”€â”€ output/
                â”‚   â”œâ”€â”€ json/
                â”‚   â””â”€â”€ parquet/
                â”œâ”€â”€ docker-compose.yml
                â””â”€â”€ README.md

